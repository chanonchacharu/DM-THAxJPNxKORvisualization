{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-21T11:56:09.448483Z","iopub.execute_input":"2023-09-21T11:56:09.449151Z","iopub.status.idle":"2023-09-21T11:56:09.510811Z","shell.execute_reply.started":"2023-09-21T11:56:09.449068Z","shell.execute_reply":"2023-09-21T11:56:09.509173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nimport seaborn as sns\n\nimport json \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:09.514120Z","iopub.execute_input":"2023-09-21T11:56:09.515209Z","iopub.status.idle":"2023-09-21T11:56:11.028632Z","shell.execute_reply.started":"2023-09-21T11:56:09.515153Z","shell.execute_reply":"2023-09-21T11:56:11.027052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing and Data Exploration\n\nPossible Analysis (at a later stage): Use the Jaccard Similarity to check whether there is difference in the top 10 to 20 popular genres and tags between dramas from South Korea, Thailand, and Japan","metadata":{}},{"cell_type":"markdown","source":"Fighting against the formitable opponent know as Korean Drama, Lakorn Thai and Japanese Drama cannot compete in terms of overall viewership. We argued that Thai and Japanese drama are niche categories of drama that are noteworthy of being classified as \"Hidden Gem of Asia\" - they are niche, but they are outstanding.\n\nGiven the context and the theory we proposed, we argued that it is not appropriate to use the total viewership to make judgement regarding the drama performance. Hence, we came up with a metric to help with the comparison. \n\nMetric: `scored_significant = total user score * total number of user that casted the vote / total people who watched the drama`","metadata":{}},{"cell_type":"code","source":"def split_data(df: pd.DataFrame) -> pd.DataFrame:\n    df['genres'] = df['genres'].apply(lambda x: [char.replace(',','').strip() for char in str(x).split(',')])\n    df['tags'] = df['tags'].apply(lambda x: [char.replace(',','').strip()  for char in str(x).split(',')])\n    df['aired_on'] = df['aired_on'].apply(lambda x: [char.replace(',','').strip()  for char in str(x).split()])\n    return df\n\n# Test \ndef split_data_test(df: pd.DataFrame) -> pd.DataFrame:\n    df['genres'] = df['genres'].apply(lambda x: [char.strip() for char in str(x).split(',')])\n    df['tags'] = df['tags'].apply(lambda x: [char.strip() for char in str(x).split(',')])\n    df['aired_on'] = df['aired_on'].apply(lambda x: [char.strip() for char in str(x).split(',')])\n    return df\n\ndef custom_rating_generator(df: pd.DataFrame) -> pd.DataFrame:\n    df['watched_ratio'] = (df['tot_num_user'] / df['tot_watched']).round(3)\n    df['scored_signif'] = df['tot_user_score'] * df['watched_ratio']\n    return df\n\ndef fillna_numerical_columns(df: pd.DataFrame) -> pd.DataFrame:\n    numerical_columns = df.describe().columns.tolist()\n    \n    for col in numerical_columns:\n        df[col] = df[col].fillna(0.0)\n        \n    return df\n\ndef drama_process_pipeline(df: pd.DataFrame) -> pd.DataFrame:\n    \n    # Filtered for prefered data\n    df['year'] = df['year'].apply(lambda x: str(x))\n    \n    df = df[\n        (df['type'].isin(['Drama','Movie'])) & \n        (df['year'].isin(['2021','2022','2023']))\n    ]\n    \n    # df = split_data(df)\n    df = split_data_test(df)\n    df = custom_rating_generator(df)\n    df = fillna_numerical_columns(df)\n    \n    removed_features = [\n        'drama_id','synopsis','rank','popularity',\n        'director','sc_writer','start_dt', 'end_dt'\n    ]\n    df = df.drop(columns=removed_features,axis=1)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:11.029881Z","iopub.execute_input":"2023-09-21T11:56:11.030475Z","iopub.status.idle":"2023-09-21T11:56:11.051155Z","shell.execute_reply.started":"2023-09-21T11:56:11.030436Z","shell.execute_reply":"2023-09-21T11:56:11.049404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/tha_drama.csv')\ntest.loc[:.1]['tags'][0].split(',')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:11.052754Z","iopub.execute_input":"2023-09-21T11:56:11.053163Z","iopub.status.idle":"2023-09-21T11:56:11.173235Z","shell.execute_reply.started":"2023-09-21T11:56:11.053092Z","shell.execute_reply":"2023-09-21T11:56:11.171842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess Drama Metadata Dataset\ndtha_df = drama_process_pipeline(pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/tha_drama.csv'))\ndkor_df = drama_process_pipeline(pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/kor_drama.csv'))\ndjap_df = drama_process_pipeline(pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/jap_drama.csv'))\n\n# Extract Drama Name\n# tha_dname = dtha_df[dtha_df['type'] == 'Drama']['drama_name'].tolist()\n# kor_dname = dkor_df[dkor_df['type'] == 'Drama']['drama_name'].tolist()\n# jap_dname = djap_df[djap_df['type'] == 'Drama']['drama_name'].tolist()\n\n# print(f'Thai: {dtha_df.shape[0]}, #drama := {len(tha_dname)}')\n# print(f'South Korea: {dkor_df.shape[0]}, #drama := {len(kor_dname)}')\n# print(f'Japan: {djap_df.shape[0]}, #drama := {len(jap_dname)}')\n\n# # Valid Drama Dicts should be placed after the preprocessing task (remove outliers etc. )\n# global valid_drama_dict \n\n# valid_drama_dict = {\n#     'Thailand': tha_dname,\n#     'South Korea': kor_dname,\n#     'Japan': jap_dname\n# }\n\n# # Load the data: ignoring drama_id, rank, and pop(ularity) because we won't be using the website ranking system\n# dtha_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/tha_drama.csv').iloc[:,1:-2]\n# dkor_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/kor_drama.csv').iloc[:,1:-2]\n# djap_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/jap_drama.csv').iloc[:,1:-2]\n\n\n# # Extract Drama Name\n# tha_dname = dtha_df[dtha_df['type'] == 'Drama']['drama_name'].tolist()\n# kor_dname = dkor_df[dkor_df['type'] == 'Drama']['drama_name'].tolist()\n# jap_dname = djap_df[djap_df['type'] == 'Drama']['drama_name'].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:11.178517Z","iopub.execute_input":"2023-09-21T11:56:11.182237Z","iopub.status.idle":"2023-09-21T11:56:11.537433Z","shell.execute_reply.started":"2023-09-21T11:56:11.182183Z","shell.execute_reply":"2023-09-21T11:56:11.536281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter for only drama\ndtha_df = dtha_df[dtha_df['type'] == 'Drama']\ndkor_df = dkor_df[dkor_df['type'] == 'Drama']\ndjap_df = djap_df[djap_df['type'] == 'Drama']\n\nprint(f'Thai: {dtha_df.shape[0]}')\nprint(f'South Korea: {dkor_df.shape[0]}')\nprint(f'Japan: {djap_df.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:11.539301Z","iopub.execute_input":"2023-09-21T11:56:11.540088Z","iopub.status.idle":"2023-09-21T11:56:11.554321Z","shell.execute_reply.started":"2023-09-21T11:56:11.540040Z","shell.execute_reply":"2023-09-21T11:56:11.553352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing drama that falls below the 25th percentile of viewership since we want to focus on more popular or the better representative drams of each region. We can consider the alternative where we collected the data with overall viewership greater than the average viewship. However, this approach will make the remaining dataset really small.","metadata":{}},{"cell_type":"code","source":"'''PROBABLY DONT NEED THIS FUNCTION GIVEN THE CURRENT ANALYSIS'''\ndef process_percentile_watched_pipeline(\n    data_frame: pd.DataFrame,\n    column_name: str,\n    percentile: float = 0.25,\n) -> pd.DataFrame:\n    \n    lower_percentile = data_frame[column_name].quantile(percentile) \n    \n    filtered_data = data_frame[data_frame[column_name] >= lower_percentile]\n    \n    removed_data = data_frame[data_frame[column_name]< lower_percentile]\n    \n    print(f\"Output report:\\nFiltered df: {filtered_data.shape[0]}\\nRemoved: {removed_data.shape[0]}\")\n    return filtered_data, removed_data","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:11.555952Z","iopub.execute_input":"2023-09-21T11:56:11.556666Z","iopub.status.idle":"2023-09-21T11:56:11.568014Z","shell.execute_reply.started":"2023-09-21T11:56:11.556630Z","shell.execute_reply":"2023-09-21T11:56:11.566418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_outliers_df_pipeline(\n    data_frame: pd.DataFrame,\n    column_name: str,\n    iqr_multiplier: float = 1.5\n) -> pd.DataFrame:\n    \n    q1 = data_frame[column_name].quantile(0.25)\n    q3 = data_frame[column_name].quantile(0.75)\n    iqr = q3 - q1\n    lower_bound = q1 - iqr_multiplier * iqr\n    upper_bound = q3 + iqr_multiplier * iqr\n\n    outliers_df = data_frame[(data_frame[column_name] < lower_bound) | (data_frame[column_name] > upper_bound)]\n    filtered_df = data_frame[(data_frame[column_name] >= lower_bound) & (data_frame[column_name] <= upper_bound)]\n    \n    print(f\"Pipeline Report:\\nOriginal df: {data_frame.shape[0]}\\nOutliers df: {outliers_df.shape[0]}\\nCleaned df: {filtered_df.shape[0]}\",end=\"\\n\\n\")\n    \n    return outliers_df, filtered_df","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:11.569626Z","iopub.execute_input":"2023-09-21T11:56:11.570667Z","iopub.status.idle":"2023-09-21T11:56:11.582697Z","shell.execute_reply.started":"2023-09-21T11:56:11.570620Z","shell.execute_reply":"2023-09-21T11:56:11.581077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess the data (removed outliers)\nprint(\"Thailand:\")\ndtha_df_outliers, dtha_df_filtered = process_outliers_df_pipeline(dtha_df, 'scored_signif', 1.5)\nprint(\"Korea:\")\ndkor_df_outliers, dkor_df_filtered = process_outliers_df_pipeline(dkor_df, 'scored_signif', 1.5)\nprint(\"Japan:\")\ndjap_df_outliers, djap_df_filtered = process_outliers_df_pipeline(djap_df, 'scored_signif', 1.5)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:11.584822Z","iopub.execute_input":"2023-09-21T11:56:11.585348Z","iopub.status.idle":"2023-09-21T11:56:11.611733Z","shell.execute_reply.started":"2023-09-21T11:56:11.585301Z","shell.execute_reply":"2023-09-21T11:56:11.610504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tha_dname = dtha_df_filtered[dtha_df_filtered['type'] == 'Drama']['drama_name'].tolist()\nkor_dname = dkor_df_filtered[dkor_df_filtered['type'] == 'Drama']['drama_name'].tolist()\njap_dname = djap_df_filtered[djap_df_filtered['type'] == 'Drama']['drama_name'].tolist()\n\nprint(f'Thai: {dtha_df.shape[0]}, #valid drama := {len(tha_dname)}')\nprint(f'South Korea: {dkor_df.shape[0]}, #valid drama := {len(kor_dname)}')\nprint(f'Japan: {djap_df.shape[0]}, #valid drama := {len(jap_dname)}')\n\n# Valid Drama Dicts should be placed after the preprocessing task (remove outliers etc. )\nglobal valid_drama_dict \n\nvalid_drama_dict = {\n    'Thailand': tha_dname,\n    'South Korea': kor_dname,\n    'Japan': jap_dname\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:11.613409Z","iopub.execute_input":"2023-09-21T11:56:11.614384Z","iopub.status.idle":"2023-09-21T11:56:11.631714Z","shell.execute_reply.started":"2023-09-21T11:56:11.614338Z","shell.execute_reply":"2023-09-21T11:56:11.630142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dkor_df_filtered['above_mean'] = np.where(\n    dkor_df_filtered['scored_signif'] >= dkor_df_filtered['scored_signif'].mean(),\n    'High','Low'\n)\n\ndtha_df_filtered['above_mean'] = np.where(\n    dtha_df_filtered['scored_signif'] >= dtha_df_filtered['scored_signif'].mean(),\n    'High','Low'\n)\n\ndjap_df_filtered['above_mean'] = np.where(\n    djap_df_filtered['scored_signif'] >= djap_df_filtered['scored_signif'].mean(),\n    'High','Low'\n)\n\ncombined_drama_df = pd.concat([dkor_df_filtered, dtha_df_filtered, djap_df_filtered], axis=0, ignore_index=True)\n\ncombined_outliers_df = pd.concat([dkor_df_outliers, dtha_df_outliers, djap_df_outliers], axis=0, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:11.633874Z","iopub.execute_input":"2023-09-21T11:56:11.634374Z","iopub.status.idle":"2023-09-21T11:56:11.661666Z","shell.execute_reply.started":"2023-09-21T11:56:11.634328Z","shell.execute_reply":"2023-09-21T11:56:11.660213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_drama_df.sample(n=3)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:11.663630Z","iopub.execute_input":"2023-09-21T11:56:11.664357Z","iopub.status.idle":"2023-09-21T11:56:11.720638Z","shell.execute_reply.started":"2023-09-21T11:56:11.664319Z","shell.execute_reply":"2023-09-21T11:56:11.719388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Beepswarm plot to show the distribution of Score Significant \n# The color shows the seperation of data where the red color belongs to the data above the mean for each country\n# Likewise, the data that is below the mean is denoted as blue color\nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize=(10,6))\n\nsns.swarmplot(x='scored_signif',y='country',\n              data=combined_drama_df[combined_drama_df['above_mean'] == 'Low'], legend=False,\n              color='blue', alpha=0.6, label='Above Mean')\n\nsns.swarmplot(x='scored_signif',y='country',hue='tot_watched', \n              data=combined_drama_df[combined_drama_df['above_mean'] == 'High'], legend=True,\n              palette='flare',alpha=1.0, label='Below Mean')\n\nsns.swarmplot(x='scored_signif',y='country',\n              data=combined_outliers_df, legend=False,\n              marker=\"x\", linewidth=2.0, label='(Removed) Outliers')\n\nplt.xlabel(\"Score Significance\")\nplt.ylabel(\"Country\")\n\nlegend_handles = [\n    mlines.Line2D([], [], color='red', label='Mean & Above', marker='o', markersize=4, linestyle='None'),\n    mlines.Line2D([], [], color='blue', label='Below Mean', marker='o', markersize=4, linestyle='None'),\n    mlines.Line2D([], [], color='black', label='(Remove) Outliers', marker='x', markersize=4, linestyle='None')\n]\n\nplt.legend(handles=legend_handles, loc='upper right', bbox_to_anchor=(1.0, 0.75), fontsize='small')\n\ntitle_text = r'$\\bf{Score}$' + ' ' + r'$\\bf{Significance}$' + '\\nDistribution by Country (from 2021-23)'\nplt.title(title_text, fontsize=12)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:11.723240Z","iopub.execute_input":"2023-09-21T11:56:11.724062Z","iopub.status.idle":"2023-09-21T11:56:19.793692Z","shell.execute_reply.started":"2023-09-21T11:56:11.724016Z","shell.execute_reply":"2023-09-21T11:56:19.792289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\n\npalette = sns.color_palette('Set2')\n\nsns.kdeplot(\n    data=combined_drama_df, x='scored_signif', hue='country',\n    fill=True, common_norm=False,\n    palette=palette, alpha=0.5, linewidth=0\n)\n\ncountry_means = combined_drama_df.groupby('country')['scored_signif'].mean()\nfor i, (country, mean) in enumerate(country_means.items()):\n    plt.axvline(mean, color=palette[i], linestyle='--', label=f'Mean ({country}): {mean:.2f}')\n\nplt.xlabel('Scored Significant')\nplt.ylabel('Density')\nplt.title('Distribution of Scored Significant by Country')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T16:03:01.938908Z","iopub.execute_input":"2023-09-21T16:03:01.939409Z","iopub.status.idle":"2023-09-21T16:03:02.508915Z","shell.execute_reply.started":"2023-09-21T16:03:01.939372Z","shell.execute_reply":"2023-09-21T16:03:02.507426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hypothesis Testing:\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom scipy.stats import skew, kurtosis\n\ndf = combined_drama_df.copy()\nkorean_data = df[df['country'] == 'South Korea']['scored_signif']\nthai_data = df[df['country'] == 'Thailand']['scored_signif']\njapan_data = df[df['country'] == 'Japan']['scored_signif']\n\n\nprint(\"KOREA\")\n# Calculate skewness and kurtosis\nskewness = skew(korean_data)\nkurt = kurtosis(korean_data)\n\n# Print the results\nprint(f\"Skewness: {skewness:.2f}\")\nprint(f\"Kurtosis: {kurt:.2f}\")\n\nprint(\"THAILAND\")\nskewness = skew(thai_data)\nkurt = kurtosis(thai_data)\n\n# Print the results\nprint(f\"Skewness: {skewness:.2f}\")\nprint(f\"Kurtosis: {kurt:.2f}\")\n\n\nprint(\"JAPAN\")\nskewness = skew(japan_data)\nkurt = kurtosis(japan_data)\n\n# Print the results\nprint(f\"Skewness: {skewness:.2f}\")\nprint(f\"Kurtosis: {kurt:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:29:54.560406Z","iopub.execute_input":"2023-09-21T14:29:54.560850Z","iopub.status.idle":"2023-09-21T14:29:54.586766Z","shell.execute_reply.started":"2023-09-21T14:29:54.560814Z","shell.execute_reply":"2023-09-21T14:29:54.584508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we say that a dataset can still be considered approximately normal for many statistical purposes despite having some deviation in skewness and kurtosis, it means that the deviations are relatively small and may not have a significant impact on the validity of certain statistical analyses that assume normality.\n\n1. **Benchmark or Range for Skewness:**\n   - A skewness value of 0 indicates a perfectly symmetric distribution (no skew).\n   - A positive skewness value indicates right skew (data stretched to the right).\n   - A negative skewness value indicates left skew (data stretched to the left).\n   - For many statistical analyses, skewness values between -1 and +1 are considered small and may not pose a problem. Beyond this range, particularly extreme skewness (e.g., greater than +2 or less than -2), may raise concerns about the assumption of normality.\n\n2. **Benchmark or Range for Kurtosis:**\n   - A kurtosis value of 3 indicates the kurtosis of a standard normal distribution (mesokurtic).\n   - A positive kurtosis value (greater than 3) indicates heavy tails compared to a normal distribution (leptokurtic).\n   - A negative kurtosis value (less than 3) indicates lighter tails compared to a normal distribution (platykurtic).\n   - Similar to skewness, for many statistical analyses, kurtosis values within a certain range (e.g., -2 to +2) may be considered small deviations from normality and may not significantly affect results. Extreme kurtosis values (much greater than +2 or much less than -2) may warrant caution.\n\nIt's important to note that the interpretation of skewness and kurtosis should consider the specific context of your analysis and the statistical method being used. Some statistical tests and methods are more robust to deviations from normality than others. Additionally, the sample size can also influence the impact of deviations on statistical results.\n\nIf your analysis relies heavily on the assumption of normality (e.g., parametric tests like t-tests or ANOVA), it's a good practice to assess normality using both statistical tests (e.g., Shapiro-Wilk test) and visual methods (e.g., Q-Q plots) in addition to examining skewness and kurtosis. If deviations from normality are substantial, you may need to consider alternative non-parametric tests or transformations of the data.\n- **In our case, the deviation isn't susbstantial; hence, we can still purposed hypothesis testing like ANOVA**\n\n","metadata":{}},{"cell_type":"markdown","source":"### Boostrapping\n\nBootstrapping should be used for hypothesis testing with skewed data because it provides more robust and reliable results by reducing the influence of outliers and non-normality, making it a valuable tool for assessing statistical significance in such scenarios.\n\nSummary:\n\nBootstrapping mitigates skewness by:\n\n1. Reducing the impact of outliers.\n2. Creating replicated datasets.\n3. Being non-parametric (no distribution assumptions).\n4. Providing confidence intervals.\n5. Facilitating hypothesis testing on skewed data.","metadata":{}},{"cell_type":"code","source":"# Reference: https://www.datatipz.com/blog/hypothesis-testing-with-bootstrapping-python\n# The method has been modified to fit with the data + Annotation and Additional Artistic Approach\nfrom tqdm.auto import tqdm\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef get_bootstrap(\n    data_column_1, \n    data_column_2, \n    iterations=15000, \n    statistic=np.mean, \n    sign_level=0.05,\n    name_1=None,\n    name_2=None\n) -> None:\n    \n    boot_len = max(len(data_column_1), len(data_column_2))\n    boot_data, samples_1_a, samples_2_a = [], [], []\n\n    # Bootstrap sampling\n    for _ in tqdm(range(iterations)):\n        sample_1, sample_2 = data_column_1.sample(boot_len, replace=True).values, data_column_2.sample(boot_len, replace=True).values\n        boot_data.append(statistic(sample_2 - sample_1))\n        samples_1_a.append(np.mean(sample_1))\n        samples_2_a.append(np.mean(sample_2))\n\n    # Calculate quantiles\n    quants = pd.DataFrame(boot_data).quantile([sign_level / 2, 1 - sign_level / 2])\n\n    # Calculate p-values\n    boot_mean, boot_std = np.mean(boot_data), np.std(boot_data)\n    p_1 = norm.cdf(0, boot_mean, boot_std)\n    p_2 = norm.cdf(0, -boot_mean, boot_std)\n    p_value = min(p_1, p_2) * 2\n\n    fig, axes = plt.subplots(2, 1, figsize=(10, 10), dpi=100)\n    \n    # Plot 1\n    axes[0].grid(True, linestyle='-', linewidth=0.7, color='lightgrey')\n    axes[0].set_facecolor('white')\n    n, _, bars = axes[0].hist(boot_data, bins=50)\n    for bar in bars:\n        if bar.get_x() <= quants.iloc[0][0] or bar.get_x() >= quants.iloc[1][0]:\n            bar.set_facecolor('palevioletred')\n        else: \n            bar.set_facecolor('deepskyblue')\n        bar.set_edgecolor('purple')\n\n    axes[0].vlines(boot_mean, ymin=0, ymax=max(n), linestyle='-', color='orange', linewidth=4.0)   \n\n    # Add annotations for p-value, boot_mean, H0, and H1\n    axes[0].annotate(f\"p = {np.round(p_value, 6)}\", xy=(0.045, 0.9), xycoords=\"axes fraction\", fontsize=12, color='red')\n    # axes[0].annotate(f\"Boot Mean = \\n{np.round(boot_mean, 6)}\", xy=(0.05, 0.75), xycoords=\"axes fraction\", fontsize=12, color='black')\n    axes[0].annotate(\"H0: The average \\nscore significance \\nis equal\", xy=(0.045, 0.55), xycoords=\"axes fraction\", fontsize=12, color='black')\n    axes[0].annotate(\"H1: The average score\\nsignificance is different\", xy=(0.75, 0.6), xycoords=\"axes fraction\", fontsize=12, color='black')\n\n    axes[0].vlines(quants, ymin=0, ymax=max(n), linestyle='--', color='purple', linewidth=2.0)\n    axes[0].annotate(f\"{quants.iloc[0][0]:.3f}\", xy=(quants.iloc[0][0], 0), xytext=(quants.iloc[0][0]+.001, max(n)/2))\n    axes[0].annotate(f\"{quants.iloc[1][0]:.3f}\", xy=(quants.iloc[1][0], 0), xytext=(quants.iloc[1][0]+.001, max(n)/2))\n    axes[0].set(xlabel='Bootstrap Data', ylabel='Frequency', title=f'Distribution of differences in {statistic.__name__}s\\nBootstrap Mean Difference: {np.round(boot_mean, 6)}')\n    \n    # Plot 2\n    axes[1].grid(True, linestyle='-', linewidth=0.7, color='lightgrey')\n    axes[1].set_facecolor('white')\n    axes[1].hist(samples_1_a, bins=50, color='deepskyblue', alpha=0.5, label=f\"{name_1}'s {statistic.__name__} (Mean={np.round(np.mean(samples_1_a), 6)})\")\n    axes[1].hist(samples_2_a, bins=50, color='crimson', alpha=0.5, label=f\"{name_2}'s {statistic.__name__} (Mean={np.round(np.mean(samples_2_a), 6)})\")\n    axes[1].set(ylabel='Frequency')\n    axes[1].legend()\n    plt.subplots_adjust(hspace=0.2)\n\n    # Result\n    conclusion = \"reject\" if p_value <= sign_level else \"fail to reject\"\n    axes[1].annotate(f\"Result: {conclusion} H0\", xy=(0.40, 0.6), xycoords=\"axes fraction\", fontsize=12, color='black')\n\ndf = combined_drama_df.copy()\nkorean_data = df[df['country'] == 'South Korea']['scored_signif']\nthai_data = df[df['country'] == 'Thailand']['scored_signif']\n\n# print(f\"Korea: {np.mean(korean_data)}, Thai: {np.mean(thai_data)}\")\nget_bootstrap(thai_data, korean_data, name_1='Thailand', name_2='South Korea')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:40:24.737885Z","iopub.execute_input":"2023-09-21T15:40:24.738372Z","iopub.status.idle":"2023-09-21T15:40:30.705037Z","shell.execute_reply.started":"2023-09-21T15:40:24.738336Z","shell.execute_reply":"2023-09-21T15:40:30.704182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = combined_drama_df.copy()\nkorean_data = df[df['country'] == 'South Korea']['scored_signif']\njap_data = df[df['country'] == 'Japan']['scored_signif']\n\nget_bootstrap(jap_data, korean_data, name_1='Japan', name_2='South Korea')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:40:30.706757Z","iopub.execute_input":"2023-09-21T15:40:30.707741Z","iopub.status.idle":"2023-09-21T15:40:37.165587Z","shell.execute_reply.started":"2023-09-21T15:40:30.707692Z","shell.execute_reply":"2023-09-21T15:40:37.164368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hypothese testing for South Korea vs. Thailand","metadata":{}},{"cell_type":"code","source":"from scipy import stats\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# South Korean Drama vs. Lakorn Thai\n\ndf = combined_drama_df.copy()\nkorean_data = df[df['country'] == 'South Korea']['scored_signif']\nthai_data = df[df['country'] == 'Thailand']['scored_signif']\n\n# Perform a two-sample t-test\nt_statistic, p_value = stats.ttest_ind(korean_data, thai_data, alternative='greater')\n\n# Set the significance level (alpha)\nalpha = 0.05\n\n# Create a range of x-values for the t-distribution curve\nx = np.linspace(-5, 5, 1000)\n\n# Create the t-distribution curve\nt_distribution = stats.t.pdf(x, len(korean_data) + len(thai_data) - 2)\n\n# Create a figure\nplt.figure(figsize=(10, 6))\n\n# Plot the t-distribution\nplt.plot(x, t_distribution, label='t-Distribution', color='blue')\n\n# Shade the region corresponding to the p-value\nif t_statistic > 0:\n    plt.fill_between(x, 0, t_distribution, where=(x >= t_statistic), color='red', alpha=0.5, label='p-Value')\nelse:\n    plt.fill_between(x, 0, t_distribution, where=(x <= -t_statistic), color='red', alpha=0.5, label='p-Value')\n\n# Add text annotations for the p-value and significance level\nif t_statistic > 6:\n    plt.text(3, 0.12, f'p = {p_value:.4f}', fontsize=12, color='red')\n    plt.text(3, 0.15, f't-statistics = {t_statistic:.4f}', fontsize=12, color='black')\nelse:\n    plt.text(t_statistic, 0.05, f'p = {p_value:.4f}', fontsize=12, color='red')\n    \nplt.axvline(\n    x=stats.t.ppf(1 - alpha / 2, len(korean_data) + len(thai_data) - 2), \n    color='green', linestyle='--', label=f'Significance Level\\n(alpha={alpha})',\n    linewidth=2.5,\n)\n\nhypothesis_text = f\"\"\"Null Hypothesis (H0): The average score significance \\nof Korean drama is equal to that of Lakorn Thai. \\n\nAlternative Hypothesis (H1): The average score significance \\nof Korean drama is greater than that of Lakorn Thai.\n\"\"\"\nplt.text(6.0, 0.3, hypothesis_text, fontsize=12, color='gray', ha='left', va='top')\n\nif p_value < alpha:\n    analysis_result = f\"Reject the null hypothesis: The average score significance \\nof Korean drama is greater than that of Lakorn Thai.\"\nelse:\n    analysis_reusult = f\"Fail to reject the null hypothesis: There is not enough evidence to conclude \\nthat the average score significance of Korean drama is greater than that of Lakorn Thai.\"\n\nplt.text(6.0, 0.15, f\"Hypothesis Testing Result:\", color='blue', ha='left', va='top')\nplt.text(6.0, 0.125, analysis_result, fontsize=12, color='black', ha='left', va='top')\n    \n# Set labels and title\nplt.xlabel('t-Values')\nplt.ylabel('Probability Density')\nplt.title('t-Distribution\\nSouth Korean Drama vs. Lakorn Thai')\nplt.legend()\nplt.grid(False)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:21.340823Z","iopub.execute_input":"2023-09-21T11:56:21.341787Z","iopub.status.idle":"2023-09-21T11:56:21.948208Z","shell.execute_reply.started":"2023-09-21T11:56:21.341735Z","shell.execute_reply":"2023-09-21T11:56:21.947195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hypothesis Testing for South Korea vs. Japan","metadata":{}},{"cell_type":"code","source":"# South Korean vs Japanese Drama\ndf = combined_drama_df.copy()\nkorean_data = df[df['country'] == 'South Korea']['scored_signif']\nthai_data = df[df['country'] == 'Japan']['scored_signif']\n\n# Perform a two-sample t-test\nt_statistic, p_value = stats.ttest_ind(korean_data, thai_data, alternative='greater')\n\n# Set the significance level (alpha)\nalpha = 0.05\n\n# Create a range of x-values for the t-distribution curve\nx = np.linspace(-5, 5, 1000)\n\n# Create the t-distribution curve\nt_distribution = stats.t.pdf(x, len(korean_data) + len(thai_data) - 2)\n\n# Create a figure\nplt.figure(figsize=(10, 6))\n\n# Plot the t-distribution\nplt.plot(x, t_distribution, label='t-Distribution', color='blue')\n\n# Shade the region corresponding to the p-value\nif t_statistic > 0:\n    plt.fill_between(x, 0, t_distribution, where=(x >= t_statistic), color='red', alpha=0.5, label='p-Value')\nelse:\n    plt.fill_between(x, 0, t_distribution, where=(x <= -t_statistic), color='red', alpha=0.5, label='p-Value')\n\n# Add text annotations for the p-value and significance level\nif t_statistic > 6:\n    plt.text(3, 0.12, f'p = {p_value:.4f}', fontsize=12, color='red')\n    plt.text(3, 0.15, f't-statistics = {t_statistic:.4f}', fontsize=12, color='black')\nelse:\n    plt.text(t_statistic, 0.05, f'p = {p_value:.4f}', fontsize=12, color='red')\n    \nplt.axvline(\n    x=stats.t.ppf(1 - alpha / 2, len(korean_data) + len(thai_data) - 2), \n    color='green', linestyle='--', label=f'Significance Level\\n(alpha={alpha})',\n    linewidth=2.5,\n)\n\nhypothesis_text = f\"\"\"Null Hypothesis (H0): The average score significance \\nof Korean drama is equal to that of Japanese Drama. \\n\nAlternative Hypothesis (H1): The average score significance \\nof Korean drama is greater than that of Japanese Drama.\n\"\"\"\nplt.text(6.0, 0.3, hypothesis_text, fontsize=12, color='gray', ha='left', va='top')\n\nif p_value < alpha:\n    analysis_result = f\"Reject the null hypothesis: The average score significance \\nof Korean drama is greater than that of Japanese Drama.\"\nelse:\n    analysis_reusult = f\"Fail to reject the null hypothesis: There is not enough evidence to conclude \\nthat the average score significance of Korean drama is greater than that of Japanese Drama.\"\n\nplt.text(6.0, 0.15, f\"Hypothesis Testing Result:\", color='blue', ha='left', va='top')\nplt.text(6.0, 0.125, analysis_result, fontsize=12, color='black', ha='left', va='top')\n    \n# Set labels and title\nplt.xlabel('t-Values')\nplt.ylabel('Probability Density')\nplt.title('t-Distribution\\nSouth Korean Drama vs. Japanese Drama')\nplt.legend()\nplt.grid(False)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:21.950160Z","iopub.execute_input":"2023-09-21T11:56:21.950849Z","iopub.status.idle":"2023-09-21T11:56:22.552266Z","shell.execute_reply.started":"2023-09-21T11:56:21.950806Z","shell.execute_reply":"2023-09-21T11:56:22.551247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ANOVA: Is there a significant difference in the means of the groups (South Kore, Thailand, and Japan)","metadata":{}},{"cell_type":"code","source":"df = combined_drama_df.copy()\ngrouped_data = [df['scored_signif'][df['country'] == country] for country in df['country'].unique()]\n\nf_statistic, p_value = stats.f_oneway(*grouped_data)\n\nalpha = 0.05\n\nx = np.linspace(0, 5, 1000)\nf_distribution = stats.f.pdf(x, len(df['country'].unique()) - 1, len(df['scored_signif']) - len(df['country'].unique()))\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, f_distribution, label='F-Distribution', color='blue')\n\nplt.fill_between(x, 0, f_distribution, where=(x >= f_statistic), color='red', alpha=0.5, label='p-Value')\n\nplt.text(3.5, 0.55, f'F-Statistic = {f_statistic:.4f}', fontsize=12, color='black')\nplt.text(3.5, 0.5, f'p = {p_value:.4f}', fontsize=12, color='red')\nplt.axvline(\n    x=stats.f.ppf(1 - alpha, len(df['country'].unique()) - 1, \n    len(df['scored_signif']) - len(df['country'].unique())), \n    color='green', linestyle='--', label=f'Significance Level\\n(alpha={alpha})'\n)\n\nhypothesis_text = f\"\"\"Null Hypothesis (H0): There is no sigificant difference \\namong the means of scored significant better \\nthree countries (μ1 = μ2 = μ3) \\n\nAlthernative Hypothesis (H1): At least one population \\nmean is different from others (μ1 ≠ μ2 ≠ μ3)\n\"\"\"\n\nplt.text(5.5, 0.7, hypothesis_text, fontsize=12, color='gray', ha='left', va='top')\n\nif p_value < alpha:\n    analysis_result = f\"Reject the null hypothesis: There is a significant\\ndifference in the means of at least two groups\"\nelse:\n    analysis_reusult = f\"Fail to reject the null hypothesis: There is no significant\\ndifference in the means of the groups\"\n    \nplt.text(5.5, 0.33, f\"Hypothesis Testing Result:\", fontsize=12, color='blue', ha='left', va='top')\nplt.text(5.5, 0.25, analysis_result, fontsize=12, color='black', ha='left', va='top')\n\nplt.xlabel('F-Values')\nplt.ylabel('Probability Density')\nplt.title('F-Distribution:\\nSouth Korea, Thai, and Japan')\nplt.legend()\nplt.grid(False)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T12:32:17.056777Z","iopub.execute_input":"2023-09-21T12:32:17.057280Z","iopub.status.idle":"2023-09-21T12:32:17.527877Z","shell.execute_reply.started":"2023-09-21T12:32:17.057242Z","shell.execute_reply":"2023-09-21T12:32:17.526375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter \n\ngenres_count_list = []\n\nfor genres in dkor_df_filtered['genres'].to_list():\n    for genre in genres:\n        genres_count_list.append(genre)\n\ngenre_df = pd.DataFrame.from_dict(Counter(genres_count_list),orient='index').rename(columns={0:'Count'})\ngenre_df.sort_values(by='Count',ascending = False,inplace = True)\ngenre_df = genre_df.reset_index().rename(columns={'index':'genre'})\ngenre_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:24.489395Z","iopub.execute_input":"2023-09-21T11:56:24.489818Z","iopub.status.idle":"2023-09-21T11:56:24.512959Z","shell.execute_reply.started":"2023-09-21T11:56:24.489781Z","shell.execute_reply":"2023-09-21T11:56:24.511538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\nfrom matplotlib.colors import LinearSegmentedColormap\n\ngenre_count_dict = dict(zip(genre_df['genre'], genre_df['Count']))\n\nwordcloud = WordCloud(\n    width=800, height=400, background_color='black', \n    contour_color = 'black', colormap = 'gray',\n    normalize_plurals=True\n).generate_from_frequencies(genre_count_dict)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')  # Remove axis labels\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:24.514744Z","iopub.execute_input":"2023-09-21T11:56:24.515242Z","iopub.status.idle":"2023-09-21T11:56:25.251613Z","shell.execute_reply.started":"2023-09-21T11:56:24.515198Z","shell.execute_reply":"2023-09-21T11:56:25.250355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------","metadata":{}},{"cell_type":"markdown","source":"# We might disregard this \"Review\" section since we couldn't get much of the information?\n\n## Reviews\n\nReviews can give a more indept analysis on why these drama are so great. However, not every drama has the same number of comments. In this case, we will just do the analysis based on the existing reviews. \n\nFor each review, the scoring can be seperated into mutliple sub-scoring; hence, each field provides more insight!","metadata":{}},{"cell_type":"markdown","source":"#### Preprocessing Task\nWe will preprocess that data. Firstly, we defined a function called `review_preprocess_pipeline` that extract useful information such as total number of episode the reviewer watched, which we can used to calculated the `watched_ratio`, This ratio will be used as a weighting factor to find weighted average rating score or `wrat_avg_score` reviewers gave to the drama.\n\n#### Why should we consider using a weighted score in our analysis? \nTo illustrate this, let's draw a parallel with shopping on an e-commerce website. Imagine you come across two identical products being sold by two different vendors. In most cases, you would likely opt to purchase from the vendor with a stronger reputation. This reputation can manifest in various ways: a higher number of followers, likes, or exceptionally high average review scores, such as a perfect 5.0 stars.\n\nNow, let's apply a similar line of thinking to our analysis. When it comes to reviews of a particular drama, we value input from individuals with greater credibility. This credibility comes from those who have already watched the entire drama, as they can provide a more comprehensive and informed perspective on it. Therefore, we may choose to assign more weight to reviews from viewers with a proven track record of completing the drama, just as we would preferentially buy from a trusted vendor when shopping online.","metadata":{}},{"cell_type":"markdown","source":"### Probability Problem: \nGiven that we picked one Lakorn Thai randomly, what is the probability that the Lakorn is top rated and belongs to Horror, Romance or  ","metadata":{}},{"cell_type":"code","source":"def review_preprocess_pipeline(\n    df: pd.DataFrame,\n    country: str\n) -> pd.DataFrame:\n    \n    # Filtered for review that is valid\n    df = df[df['title'].isin(valid_drama_dict[country])].iloc[:,1:]\n    \n    # Create Additional Features into each Dataset as a Key to determine the associated Country of the Drama being commented\n    df['country'] = country\n    \n    # Removed the people that watched more than the number of episodes?\n    \n    # Extract Number of Epsiode watched and Total Episode Number\n    df['ep_watched'] = df['ep_watched'].fillna('0 of 0 episodes seen')\n    df['tot_watched'] = df['ep_watched'].apply(lambda x: int(str(x).split(' ')[0]))\n    df['tot_ep'] = df['ep_watched'].apply(lambda x: int(str(x).split(' ')[2]))\n    \n    # Feature Engineering\n    df['ep_watched_ratio'] = df['tot_watched'] / df['tot_ep']\n    df['ep_watched_ratio'] = df['ep_watched_ratio'].fillna(0.0).apply(lambda x: round(x,3))\n    \n    df['avg_score'] = df[['story','acting_cast','music','rewatch_value']].mean(axis=1)\n    df['wrat_avg_score'] = df['avg_score'] * df['ep_watched_ratio']\n    \n    # Remove Features that wouldn't be part of the analysis \n    df = df.drop(columns=['ep_watched', 'text'], axis=1)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:25.253054Z","iopub.execute_input":"2023-09-21T11:56:25.253464Z","iopub.status.idle":"2023-09-21T11:56:25.265763Z","shell.execute_reply.started":"2023-09-21T11:56:25.253429Z","shell.execute_reply":"2023-09-21T11:56:25.264395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rtha_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/tha_user_reviews.csv')\nrkor_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/kor_user_reviews.csv')\nrjap_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/jap_user_reviews.csv')\n\nrtha_df_f = review_preprocess_pipeline(rtha_df,'Thailand')\nrkor_df_f = review_preprocess_pipeline(rkor_df, 'South Korea')\nrjap_df_f = review_preprocess_pipeline(rjap_df, 'Japan')\n\nrtha_df_f.shape, rkor_df_f.shape, rjap_df_f.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:25.267304Z","iopub.execute_input":"2023-09-21T11:56:25.267720Z","iopub.status.idle":"2023-09-21T11:56:26.320773Z","shell.execute_reply.started":"2023-09-21T11:56:25.267673Z","shell.execute_reply":"2023-09-21T11:56:26.319596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rtha_outliers, rtha_df_filtered = process_outliers_df_pipeline(rtha_df_f, 'wrat_avg_score')\nrkor_outliers, rkor_df_filtered = process_outliers_df_pipeline(rkor_df_f, 'wrat_avg_score')\nrjap_outliers, rjap_df_filtered = process_outliers_df_pipeline(rjap_df_f, 'wrat_avg_score')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:26.322679Z","iopub.execute_input":"2023-09-21T11:56:26.323444Z","iopub.status.idle":"2023-09-21T11:56:26.347827Z","shell.execute_reply.started":"2023-09-21T11:56:26.323398Z","shell.execute_reply":"2023-09-21T11:56:26.346380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rtha_df_filtered.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:26.349645Z","iopub.execute_input":"2023-09-21T11:56:26.350156Z","iopub.status.idle":"2023-09-21T11:56:26.378878Z","shell.execute_reply.started":"2023-09-21T11:56:26.350080Z","shell.execute_reply":"2023-09-21T11:56:26.377696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_boxplots_with_annotations(\n    dataframe: pd.DataFrame,\n    country: str = '',\n) -> None:\n    sns.reset_orig()\n    \n    num_columns = len(dataframe.columns)\n    num_rows = (num_columns + 2) // 3\n    \n    fig, axes = plt.subplots(nrows=num_rows, ncols=3, figsize=(16, 6 * num_rows // 2))\n    axes = axes.flatten()\n\n    for i, col in enumerate(dataframe.columns):\n        # Create a boxplot for the current column in the corresponding subplot\n        ax = axes[i]\n        ax.boxplot(dataframe[col], vert=False, sym='b.', \n                   whis=1.5, patch_artist=True, boxprops=dict(facecolor='lightblue'))\n\n        # Outlier Calculation using IQR Method\n        q1 = np.percentile(dataframe[col], 25)\n        q3 = np.percentile(dataframe[col], 75)\n        iqr = q3 - q1\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n        outliers = [x for x in dataframe[col] if x < lower_bound or x > upper_bound]\n        num_outliers = len(outliers)\n        \n        mean = np.mean(dataframe[col])\n        std_dev = np.std(dataframe[col])\n        annotation = f\"Mean: {mean:.2f}\\nStd Dev: {std_dev:.2f}\\nOutliers: {num_outliers}\"\n        ax.text(0.62, 0.80, annotation, transform=ax.transAxes, fontsize=12,\n                verticalalignment='center')\n        \n        min_score = np.min(dataframe[col])\n        max_score = np.max(dataframe[col])\n        minmax_annotation = f\"Min: {min_score:.2f}\\nMax: {max_score:.2f}\"\n        ax.text(0.62, 0.25, minmax_annotation, transform=ax.transAxes, fontsize=12,\n               verticalalignment='center')\n\n        ax.set_title(col)\n    \n    big_title = f\"{country} Boxplots\"\n    fig.suptitle(big_title, fontsize=16, y=1.02)\n    \n    for j in range(num_columns, len(axes)):\n        fig.delaxes(axes[j])\n        \n    plt.tight_layout()\n    plt.show()\n\n# features = [\n#     'story', 'acting_cast', 'music', 'rewatch_value', 'overall',\n#     'n_helpful', 'tot_watched', 'tot_ep', 'ep_watched_ratio',\n#     'avg_score', 'wrat_avg_score'\n# ]\n\nfeatures = ['ep_watched_ratio','overall','wrat_avg_score']\n\ndraw_boxplots_with_annotations(rtha_df_filtered[features], 'Thailand')\n\ndraw_boxplots_with_annotations(rkor_df_filtered[features], 'South Korea')\n\ndraw_boxplots_with_annotations(rjap_df_filtered[features], 'Japan')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:26.380497Z","iopub.execute_input":"2023-09-21T11:56:26.380848Z","iopub.status.idle":"2023-09-21T11:56:28.402967Z","shell.execute_reply.started":"2023-09-21T11:56:26.380818Z","shell.execute_reply":"2023-09-21T11:56:28.401269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do we have to manage the outliers based on ep_watched_ratio, n_helpful, etc.?\n\nWhether to remove the outliers before or after performing a feature engineering like weighted averge score depends on the specific golas and requirement of this analysis. Upon observation of the fields related to score, we notice some outliers. However, this is something we would like to capture. The minimum and maximum values for these fields are between 1 and 10; hence, there is no hidden error in the data extraction or scoring process on the website. They are valuable insights even if they were outliers; henceforth, we won't remove it for this moment. \n\n\nAddress other features and how does outlier in those feature effect the final weighted score\n","metadata":{}},{"cell_type":"code","source":"print(\"Removed based the the Episode Watched Ratio: \")\n_, rtha_df_f2 = process_outliers_df_pipeline(rtha_df_filtered, 'ep_watched_ratio')\n_, rkor_df_f2 = process_outliers_df_pipeline(rkor_df_filtered, 'ep_watched_ratio')\n_, rjap_df_f2 = process_outliers_df_pipeline(rjap_df_filtered, 'ep_watched_ratio')\n\nprint(\"Removed based on Weigthed Ratio Average Score: \")\n_, rtha_df_f2 = process_outliers_df_pipeline(rtha_df_f2, 'wrat_avg_score')\n_, rkor_df_f2 = process_outliers_df_pipeline(rtha_df_f2, 'wrat_avg_score')\n_, rjap_df_f2 = process_outliers_df_pipeline(rtha_df_f2, 'wrat_avg_score')\n\n\n\nfeatures = ['ep_watched_ratio','overall','wrat_avg_score']\n\ndraw_boxplots_with_annotations(rtha_df_f2[features], 'Thailand')\n\ndraw_boxplots_with_annotations(rkor_df_f2[features], 'South Korea')\n\ndraw_boxplots_with_annotations(rjap_df_f2[features], 'Japan')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:28.404679Z","iopub.execute_input":"2023-09-21T11:56:28.405373Z","iopub.status.idle":"2023-09-21T11:56:30.179049Z","shell.execute_reply.started":"2023-09-21T11:56:28.405334Z","shell.execute_reply":"2023-09-21T11:56:30.177622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rtha_df_f2[rtha_df_f2['title'] == '10 Years Ticket']","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:30.181211Z","iopub.execute_input":"2023-09-21T11:56:30.181710Z","iopub.status.idle":"2023-09-21T11:56:30.214527Z","shell.execute_reply.started":"2023-09-21T11:56:30.181659Z","shell.execute_reply":"2023-09-21T11:56:30.212618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing the aggregate function\nagg_rtha_df = (\n    rtha_df_f2\n    .reset_index()\n    .groupby(['title'])\n    .agg({'wrat_avg_score':'mean', 'index':'count'})\n    .rename(columns={'wrat_avg_score': 'avr_wrat_score', 'index':'num_reviews'})\n    .reset_index()\n)\n\nagg_rtha_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:30.216194Z","iopub.execute_input":"2023-09-21T11:56:30.217160Z","iopub.status.idle":"2023-09-21T11:56:30.247916Z","shell.execute_reply.started":"2023-09-21T11:56:30.217094Z","shell.execute_reply":"2023-09-21T11:56:30.246404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----","metadata":{}},{"cell_type":"markdown","source":"## Actors Processing","metadata":{}},{"cell_type":"code","source":"atha_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/tha_tha_actors.csv')\nakor_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/kor_kor_actors.csv')\najap_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/jap_jap_actors.csv')\n\natha_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:30.249722Z","iopub.execute_input":"2023-09-21T11:56:30.251002Z","iopub.status.idle":"2023-09-21T11:56:30.401803Z","shell.execute_reply.started":"2023-09-21T11:56:30.250950Z","shell.execute_reply":"2023-09-21T11:56:30.400549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function to check uniqueness of the actor name\ndef check_actor_name_uniqueness(df: pd.DataFrame) -> None:\n    \n    actor_dict = {}\n    \n    for index, row in df.iterrows():\n        actor_id, actor_name = row['actor_id'], row['actor_name']\n\n        if actor_name not in actor_dict.keys():\n            actor_dict[actor_name] = {\n                'id_list': []\n            }\n\n        if actor_id not in actor_dict[actor_name]['id_list']:\n            actor_dict[actor_name]['id_list'].append(actor_id)\n\n    for key, val in actor_dict.items():\n        actor_dict[key]['nunique'] = len(actor_dict[key]['id_list'])\n\n        if actor_dict[key]['nunique'] != 1:\n            print(\"Actor name isn't unique!\")\n            return \n    \n    print(\"Actor name is UNIQUE!\")\n    return \n\n# Check the uniqueness of the actor name. Need to use them as key later-on\ncheck_actor_name_uniqueness(atha_df)\ncheck_actor_name_uniqueness(akor_df)\ncheck_actor_name_uniqueness(ajap_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:30.403104Z","iopub.execute_input":"2023-09-21T11:56:30.403813Z","iopub.status.idle":"2023-09-21T11:56:32.973866Z","shell.execute_reply.started":"2023-09-21T11:56:30.403776Z","shell.execute_reply":"2023-09-21T11:56:32.972595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write a function to merge the information of actor and drama (might have to be the new dataframe created from the processed reviews?) to get the information regarding the drama performance that each actor acted in. If have to use the processed data from review, then move code blocks related to actors to be after the user reviews sections","metadata":{}},{"cell_type":"code","source":"df = atha_df.copy()\n\ndrama_df = dtha_df_filtered[['drama_name','genres','tags','country','content_rt','scored_signif']].copy()\ncountry = 'Thailand'\n\ndf = df.drop(columns=['actor_id','character_name'],axis=1)\ndf = df[df['drama_name'].isin(valid_drama_dict[country])]\n\nmerged_df = df.merge(drama_df, on='drama_name', how='left')\n\nmerged_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:32.975492Z","iopub.execute_input":"2023-09-21T11:56:32.975859Z","iopub.status.idle":"2023-09-21T11:56:33.018700Z","shell.execute_reply.started":"2023-09-21T11:56:32.975826Z","shell.execute_reply":"2023-09-21T11:56:33.017521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df.groupby(['actor_name']).count()[['drama_name']].sort_values(['drama_name'], ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:33.020994Z","iopub.execute_input":"2023-09-21T11:56:33.021518Z","iopub.status.idle":"2023-09-21T11:56:33.045519Z","shell.execute_reply.started":"2023-09-21T11:56:33.021473Z","shell.execute_reply":"2023-09-21T11:56:33.043753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df[merged_df['actor_name'] == 'Pond Naravit Lertratkosum']","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:33.047884Z","iopub.execute_input":"2023-09-21T11:56:33.048288Z","iopub.status.idle":"2023-09-21T11:56:33.073954Z","shell.execute_reply.started":"2023-09-21T11:56:33.048254Z","shell.execute_reply":"2023-09-21T11:56:33.072335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Popular Actor with Main role based on Average Significane Score of the Drama they perforemd in\n# Create Pipeline...\nactor_score_df = merged_df.groupby(['actor_name','role']).agg({'scored_signif':'mean'}).rename(columns={'scored_signif':'avg_score'}).reset_index()\nactor_score_df[actor_score_df['role'] == 'Main Role'].sort_values(['avg_score'], ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:33.075935Z","iopub.execute_input":"2023-09-21T11:56:33.077111Z","iopub.status.idle":"2023-09-21T11:56:33.110545Z","shell.execute_reply.started":"2023-09-21T11:56:33.077059Z","shell.execute_reply":"2023-09-21T11:56:33.109225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nDictionary Architectures:\n\nactor_name\n    - role\n        - genres\n        - tags\n        - content_rt\n    - appearance\n\nval: scored_signig\n'''\n# Convert this dictionary into dataframe\nactor_meta_db = {}\n\n# Focus Only on Main Role and Support Role\nsdf = merged_df[merged_df['role'].isin(['Main Role','Support Role'])]\n\nfor index, row in sdf.iterrows():\n    \n    name, role, score = row['actor_name'], row['role'], row['scored_signif']\n    \n    if name not in actor_meta_db.keys():\n        actor_meta_db[name] = {\n            'Main Role': {},\n            'Support Role': {}\n        }\n\n    for genre in row['genres']:\n        if genre != 'nan' and genre not in actor_meta_db[name][role].keys():\n            actor_meta_db[name][role][genre] = {\n                'score_db': [],\n                'count': 0,\n            }\n            \n        if genre in actor_meta_db[name][role].keys():\n            actor_meta_db[name][role][genre]['score_db'].append(score)\n            \n            actor_meta_db[name][role][genre]['count'] += 1\n\n# import json \n# print(json.dumps(actor_meta_db, indent=4))\n            \nfor key, item in actor_meta_db.items():\n\n    for role in ['Main Role', 'Support Role']:\n        if item[role].keys():\n            for genre, (score_list_db, counter) in item[role].items():\n                average_score = np.mean(item[role][genre][score_list_db])\n\n                actor_meta_db[key][role][genre][score_list_db] = round(average_score,3)\n    \n# import json \n# print(json.dumps(actor_meta_db, indent=4))\n\n# Initialize empty lists to store data\nactor_names = []\nroles = []\ngenres = []\naverage_scores = []\nnum_appearance = []\n\n# Iterate through the dictionary to extract data\nfor actor_name, roles_data in actor_meta_db.items():\n    for role, genres_data in roles_data.items():\n        for genre, data in genres_data.items():\n            actor_names.append(actor_name)\n            roles.append(role)\n            genres.append(genre)\n            average_scores.append(data['score_db'])\n            num_appearance.append(data['count'])\n\n# Create a Pandas DataFrame\ndf = pd.DataFrame({\n    'actor_name': actor_names,\n    'role': roles,\n    'genres': genres,\n    'average_score': average_scores,\n    'num_appearance': num_appearance,\n})\n\n# Print the DataFrame\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:33.112900Z","iopub.execute_input":"2023-09-21T11:56:33.113713Z","iopub.status.idle":"2023-09-21T11:56:33.581741Z","shell.execute_reply.started":"2023-09-21T11:56:33.113663Z","shell.execute_reply":"2023-09-21T11:56:33.579998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nDictionary Architectures:\n\nactor_name\n    - role\n        - genres\n        - tags\n        - content_rt\n    - appearance\n\nval: scored_signig\n'''\n# Convert this dictionary into dataframe\nactor_meta_db = {}\n\n# Focus Only on Main Role and Support Role\nsdf = merged_df[merged_df['role'].isin(['Main Role','Support Role'])]\n\nfor index, row in sdf.iterrows():\n    \n    name, role, score = row['actor_name'], row['role'], row['scored_signif']\n    \n    if name not in actor_meta_db.keys():\n        actor_meta_db[name] = {\n            'Main Role': {},\n            'Support Role': {}\n        }\n\n    for tag in row['tags']:\n        if tag != \"(Vote or add tags)\" and genre not in actor_meta_db[name][role].keys():\n            actor_meta_db[name][role][tag] = {\n                'score_db': [],\n                'count': 0,\n            }\n            \n        if tag in actor_meta_db[name][role].keys():\n            actor_meta_db[name][role][tag]['score_db'].append(score)\n            \n            actor_meta_db[name][role][tag]['count'] += 1\n\n# import json \n# print(json.dumps(actor_meta_db, indent=4))\n            \nfor key, item in actor_meta_db.items():\n\n    for role in ['Main Role', 'Support Role']:\n        if item[role].keys():\n            for tag, (score_list_db, counter) in item[role].items():\n                average_score = np.mean(item[role][tag][score_list_db])\n\n                actor_meta_db[key][role][tag][score_list_db] = round(average_score,3)\n    \n# import json \n# print(json.dumps(actor_meta_db, indent=4))\n\n# Initialize empty lists to store data\nactor_names = []\nroles = []\ntags = []\naverage_scores = []\nnum_appearance = []\n\n# Iterate through the dictionary to extract data\nfor actor_name, roles_data in actor_meta_db.items():\n    for role, genres_data in roles_data.items():\n        for genre, data in genres_data.items():\n            actor_names.append(actor_name)\n            roles.append(role)\n            tags.append(genre)\n            average_scores.append(data['score_db'])\n            num_appearance.append(data['count'])\n\n# Create a Pandas DataFrame\ndf = pd.DataFrame({\n    'actor_name': actor_names,\n    'role': roles,\n    'tags': tags,\n    'average_score': average_scores,\n    'num_appearance': num_appearance,\n})\n\n# Print the DataFrame\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:33.583669Z","iopub.execute_input":"2023-09-21T11:56:33.584163Z","iopub.status.idle":"2023-09-21T11:56:34.431850Z","shell.execute_reply.started":"2023-09-21T11:56:33.584089Z","shell.execute_reply":"2023-09-21T11:56:34.430411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[(df['actor_name'] == 'Mai Davika Hoorne') & (df['tags'] == 'Prolonged Nudity')]","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:34.439142Z","iopub.execute_input":"2023-09-21T11:56:34.439585Z","iopub.status.idle":"2023-09-21T11:56:34.464198Z","shell.execute_reply.started":"2023-09-21T11:56:34.439549Z","shell.execute_reply":"2023-09-21T11:56:34.462683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['actor_name','tags','average_score']].groupby(['tags']).agg({'average_score':'mean'}).reset_index().sort_values('average_score',ascending=False).head(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:34.466055Z","iopub.execute_input":"2023-09-21T11:56:34.466628Z","iopub.status.idle":"2023-09-21T11:56:34.496024Z","shell.execute_reply.started":"2023-09-21T11:56:34.466572Z","shell.execute_reply":"2023-09-21T11:56:34.494638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nDictionary Architectures:\n\nactor_name\n    - role\n        - genres\n        - tags\n        - content_rt\n    - appearance\n\nval: scored_signig\n'''\n# For Tags\nactor_meta_db = {}\n\n# Focus Only on Main Role and Support Role\nsdf = merged_df[merged_df['role'].isin(['Main Role','Support Role'])]\n\nfor index, row in sdf.iterrows():\n    \n    name, role, score = row['actor_name'], row['role'], row['scored_signif']\n    \n    if name not in actor_meta_db.keys():\n        actor_meta_db[name] = {\n            'Main Role': {},\n            'Support Role': {}\n        }\n    \n    for tag in row['tags']:\n        if tag != \"(Vote or add tags)\" and tag not in actor_meta_db[name][role].keys():\n            actor_meta_db[name][role][tag] = []\n\n        if tag in actor_meta_db[name][role].keys():\n            actor_meta_db[name][role][tag].append(score)\n\nfor key, item in actor_meta_db.items():\n\n    for role in ['Main Role', 'Support Role']:\n        if item[role].keys():\n            for tag, score_list in item[role].items():\n                average_score = np.mean(score_list)\n\n                actor_meta_db[key][role][tag] = round(average_score,3)\n    \n    \nimport json\n# print(json.dumps(actor_meta_db, indent=4))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:56:34.498495Z","iopub.execute_input":"2023-09-21T11:56:34.499236Z","iopub.status.idle":"2023-09-21T11:56:35.259074Z","shell.execute_reply.started":"2023-09-21T11:56:34.499185Z","shell.execute_reply":"2023-09-21T11:56:35.257698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}