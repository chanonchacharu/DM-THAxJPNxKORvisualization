{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-20T08:02:10.170771Z","iopub.execute_input":"2023-09-20T08:02:10.171161Z","iopub.status.idle":"2023-09-20T08:02:10.184116Z","shell.execute_reply.started":"2023-09-20T08:02:10.171118Z","shell.execute_reply":"2023-09-20T08:02:10.183264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nimport seaborn as sns\n\nimport json \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:10.189094Z","iopub.execute_input":"2023-09-20T08:02:10.189381Z","iopub.status.idle":"2023-09-20T08:02:10.979935Z","shell.execute_reply.started":"2023-09-20T08:02:10.189357Z","shell.execute_reply":"2023-09-20T08:02:10.979232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing and Data Exploration\n\nPossible Analysis (at a later stage): Use the Jaccard Similarity to check whether there is difference in the top 10 to 20 popular genres and tags between dramas from South Korea, Thailand, and Japan","metadata":{}},{"cell_type":"markdown","source":"Fighting against the formitable opponent know as Korean Drama, Lakorn Thai and Japanese Drama cannot compete in terms of overall viewership. We argued that Thai and Japanese drama are niche categories of drama that are noteworthy of being classified as \"Hidden Gem of Asia\" - they are niche, but they are outstanding.\n\nGiven the context and the theory we proposed, we argued that it is not appropriate to use the total viewership to make judgement regarding the drama performance. Hence, we came up with a metric to help with the comparison. \n\nMetric: `scored_significant = total user score * total number of user that casted the vote / total people who watched the drama`","metadata":{}},{"cell_type":"code","source":"def split_data(df: pd.DataFrame) -> pd.DataFrame:\n    df['genres'] = df['genres'].apply(lambda x: [char.replace(',','').strip() for char in str(x).split(',')])\n    df['tags'] = df['tags'].apply(lambda x: [char.replace(',','').strip()  for char in str(x).split(',')])\n    df['aired_on'] = df['aired_on'].apply(lambda x: [char.replace(',','').strip()  for char in str(x).split()])\n    return df\n\n# Test \ndef split_data_test(df: pd.DataFrame) -> pd.DataFrame:\n    df['genres'] = df['genres'].apply(lambda x: [char.strip() for char in str(x).split(',')])\n    df['tags'] = df['tags'].apply(lambda x: [char.strip() for char in str(x).split(',')])\n    df['aired_on'] = df['aired_on'].apply(lambda x: [char.strip() for char in str(x).split(',')])\n    return df\n\ndef custom_rating_generator(df: pd.DataFrame) -> pd.DataFrame:\n    df['watched_ratio'] = (df['tot_num_user'] / df['tot_watched']).round(3)\n    df['scored_signif'] = df['tot_user_score'] * df['watched_ratio']\n    return df\n\ndef fillna_numerical_columns(df: pd.DataFrame) -> pd.DataFrame:\n    numerical_columns = df.describe().columns.tolist()\n    \n    for col in numerical_columns:\n        df[col] = df[col].fillna(0.0)\n        \n    return df\n\ndef drama_process_pipeline(df: pd.DataFrame) -> pd.DataFrame:\n    \n    # Filtered for prefered data\n    df['year'] = df['year'].apply(lambda x: str(x))\n    \n    df = df[\n        (df['type'].isin(['Drama','Movie'])) & \n        (df['year'].isin(['2021','2022','2023']))\n    ]\n    \n    # df = split_data(df)\n    df = split_data_test(df)\n    df = custom_rating_generator(df)\n    df = fillna_numerical_columns(df)\n    \n    removed_features = [\n        'drama_id','synopsis','rank','popularity',\n        'director','sc_writer','start_dt', 'end_dt'\n    ]\n    df = df.drop(columns=removed_features,axis=1)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:10.980993Z","iopub.execute_input":"2023-09-20T08:02:10.981596Z","iopub.status.idle":"2023-09-20T08:02:10.996383Z","shell.execute_reply.started":"2023-09-20T08:02:10.981568Z","shell.execute_reply":"2023-09-20T08:02:10.995451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/tha_drama.csv')\ntest.loc[:.1]['tags'][0].split(',')","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:10.998788Z","iopub.execute_input":"2023-09-20T08:02:10.999047Z","iopub.status.idle":"2023-09-20T08:02:11.051736Z","shell.execute_reply.started":"2023-09-20T08:02:10.999024Z","shell.execute_reply":"2023-09-20T08:02:11.050768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess Drama Metadata Dataset\ndtha_df = drama_process_pipeline(pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/tha_drama.csv'))\ndkor_df = drama_process_pipeline(pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/kor_drama.csv'))\ndjap_df = drama_process_pipeline(pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/jap_drama.csv'))\n\n# Extract Drama Name\n# tha_dname = dtha_df[dtha_df['type'] == 'Drama']['drama_name'].tolist()\n# kor_dname = dkor_df[dkor_df['type'] == 'Drama']['drama_name'].tolist()\n# jap_dname = djap_df[djap_df['type'] == 'Drama']['drama_name'].tolist()\n\n# print(f'Thai: {dtha_df.shape[0]}, #drama := {len(tha_dname)}')\n# print(f'South Korea: {dkor_df.shape[0]}, #drama := {len(kor_dname)}')\n# print(f'Japan: {djap_df.shape[0]}, #drama := {len(jap_dname)}')\n\n# # Valid Drama Dicts should be placed after the preprocessing task (remove outliers etc. )\n# global valid_drama_dict \n\n# valid_drama_dict = {\n#     'Thailand': tha_dname,\n#     'South Korea': kor_dname,\n#     'Japan': jap_dname\n# }\n\n# # Load the data: ignoring drama_id, rank, and pop(ularity) because we won't be using the website ranking system\n# dtha_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/tha_drama.csv').iloc[:,1:-2]\n# dkor_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/kor_drama.csv').iloc[:,1:-2]\n# djap_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/jap_drama.csv').iloc[:,1:-2]\n\n\n# # Extract Drama Name\n# tha_dname = dtha_df[dtha_df['type'] == 'Drama']['drama_name'].tolist()\n# kor_dname = dkor_df[dkor_df['type'] == 'Drama']['drama_name'].tolist()\n# jap_dname = djap_df[djap_df['type'] == 'Drama']['drama_name'].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:11.052767Z","iopub.execute_input":"2023-09-20T08:02:11.053014Z","iopub.status.idle":"2023-09-20T08:02:11.292457Z","shell.execute_reply.started":"2023-09-20T08:02:11.052991Z","shell.execute_reply":"2023-09-20T08:02:11.291687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter for only drama\ndtha_df = dtha_df[dtha_df['type'] == 'Drama']\ndkor_df = dkor_df[dkor_df['type'] == 'Drama']\ndjap_df = djap_df[djap_df['type'] == 'Drama']\n\nprint(f'Thai: {dtha_df.shape[0]}')\nprint(f'South Korea: {dkor_df.shape[0]}')\nprint(f'Japan: {djap_df.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:11.293452Z","iopub.execute_input":"2023-09-20T08:02:11.293697Z","iopub.status.idle":"2023-09-20T08:02:11.303114Z","shell.execute_reply.started":"2023-09-20T08:02:11.293675Z","shell.execute_reply":"2023-09-20T08:02:11.302309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing drama that falls below the 25th percentile of viewership since we want to focus on more popular or the better representative drams of each region. We can consider the alternative where we collected the data with overall viewership greater than the average viewship. However, this approach will make the remaining dataset really small.","metadata":{}},{"cell_type":"code","source":"'''PROBABLY DONT NEED THIS FUNCTION GIVEN THE CURRENT ANALYSIS'''\ndef process_percentile_watched_pipeline(\n    data_frame: pd.DataFrame,\n    column_name: str,\n    percentile: float = 0.25,\n) -> pd.DataFrame:\n    \n    lower_percentile = data_frame[column_name].quantile(percentile) \n    \n    filtered_data = data_frame[data_frame[column_name] >= lower_percentile]\n    \n    removed_data = data_frame[data_frame[column_name]< lower_percentile]\n    \n    print(f\"Output report:\\nFiltered df: {filtered_data.shape[0]}\\nRemoved: {removed_data.shape[0]}\")\n    return filtered_data, removed_data","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:11.304432Z","iopub.execute_input":"2023-09-20T08:02:11.304661Z","iopub.status.idle":"2023-09-20T08:02:11.317625Z","shell.execute_reply.started":"2023-09-20T08:02:11.304641Z","shell.execute_reply":"2023-09-20T08:02:11.316963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_outliers_df_pipeline(\n    data_frame: pd.DataFrame,\n    column_name: str,\n    iqr_multiplier: float = 1.5\n) -> pd.DataFrame:\n    \n    q1 = data_frame[column_name].quantile(0.25)\n    q3 = data_frame[column_name].quantile(0.75)\n    iqr = q3 - q1\n    lower_bound = q1 - iqr_multiplier * iqr\n    upper_bound = q3 + iqr_multiplier * iqr\n\n    outliers_df = data_frame[(data_frame[column_name] < lower_bound) | (data_frame[column_name] > upper_bound)]\n    filtered_df = data_frame[(data_frame[column_name] >= lower_bound) & (data_frame[column_name] <= upper_bound)]\n    \n    print(f\"Pipeline Report:\\nOriginal df: {data_frame.shape[0]}\\nOutliers df: {outliers_df.shape[0]}\\nCleaned df: {filtered_df.shape[0]}\",end=\"\\n\\n\")\n    \n    return outliers_df, filtered_df","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:11.320117Z","iopub.execute_input":"2023-09-20T08:02:11.320585Z","iopub.status.idle":"2023-09-20T08:02:11.331507Z","shell.execute_reply.started":"2023-09-20T08:02:11.320560Z","shell.execute_reply":"2023-09-20T08:02:11.330767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess the data (removed outliers)\nprint(\"Thailand:\")\ndtha_df_outliers, dtha_df_filtered = process_outliers_df_pipeline(dtha_df, 'scored_signif', 1.5)\nprint(\"Korea:\")\ndkor_df_outliers, dkor_df_filtered = process_outliers_df_pipeline(dkor_df, 'scored_signif', 1.5)\nprint(\"Japan:\")\ndjap_df_outliers, djap_df_filtered = process_outliers_df_pipeline(djap_df, 'scored_signif', 1.5)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:11.332483Z","iopub.execute_input":"2023-09-20T08:02:11.332771Z","iopub.status.idle":"2023-09-20T08:02:11.354377Z","shell.execute_reply.started":"2023-09-20T08:02:11.332748Z","shell.execute_reply":"2023-09-20T08:02:11.353498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tha_dname = dtha_df_filtered[dtha_df_filtered['type'] == 'Drama']['drama_name'].tolist()\nkor_dname = dkor_df_filtered[dkor_df_filtered['type'] == 'Drama']['drama_name'].tolist()\njap_dname = djap_df_filtered[djap_df_filtered['type'] == 'Drama']['drama_name'].tolist()\n\nprint(f'Thai: {dtha_df.shape[0]}, #valid drama := {len(tha_dname)}')\nprint(f'South Korea: {dkor_df.shape[0]}, #valid drama := {len(kor_dname)}')\nprint(f'Japan: {djap_df.shape[0]}, #valid drama := {len(jap_dname)}')\n\n# Valid Drama Dicts should be placed after the preprocessing task (remove outliers etc. )\nglobal valid_drama_dict \n\nvalid_drama_dict = {\n    'Thailand': tha_dname,\n    'South Korea': kor_dname,\n    'Japan': jap_dname\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:11.355257Z","iopub.execute_input":"2023-09-20T08:02:11.355867Z","iopub.status.idle":"2023-09-20T08:02:11.365696Z","shell.execute_reply.started":"2023-09-20T08:02:11.355842Z","shell.execute_reply":"2023-09-20T08:02:11.364896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dkor_df_filtered['above_mean'] = np.where(\n    dkor_df_filtered['scored_signif'] >= dkor_df_filtered['scored_signif'].mean(),\n    'High','Low'\n)\n\ndtha_df_filtered['above_mean'] = np.where(\n    dtha_df_filtered['scored_signif'] >= dtha_df_filtered['scored_signif'].mean(),\n    'High','Low'\n)\n\ndjap_df_filtered['above_mean'] = np.where(\n    djap_df_filtered['scored_signif'] >= djap_df_filtered['scored_signif'].mean(),\n    'High','Low'\n)\n\ncombined_drama_df = pd.concat([dkor_df_filtered, dtha_df_filtered, djap_df_filtered], axis=0, ignore_index=True)\n\ncombined_outliers_df = pd.concat([dkor_df_outliers, dtha_df_outliers, djap_df_outliers], axis=0, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:11.367042Z","iopub.execute_input":"2023-09-20T08:02:11.367354Z","iopub.status.idle":"2023-09-20T08:02:11.384765Z","shell.execute_reply.started":"2023-09-20T08:02:11.367329Z","shell.execute_reply":"2023-09-20T08:02:11.383750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_drama_df.sample(n=3)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:11.385831Z","iopub.execute_input":"2023-09-20T08:02:11.386094Z","iopub.status.idle":"2023-09-20T08:02:11.413470Z","shell.execute_reply.started":"2023-09-20T08:02:11.386071Z","shell.execute_reply":"2023-09-20T08:02:11.412592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Beepswarm plot to show the distribution of Score Significant \n# The color shows the seperation of data where the red color belongs to the data above the mean for each country\n# Likewise, the data that is below the mean is denoted as blue color\nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize=(10,6))\n\nsns.swarmplot(x='scored_signif',y='country',\n              data=combined_drama_df[combined_drama_df['above_mean'] == 'Low'], legend=False,\n              color='blue', alpha=0.6, label='Above Mean')\n\nsns.swarmplot(x='scored_signif',y='country',hue='tot_watched', \n              data=combined_drama_df[combined_drama_df['above_mean'] == 'High'], legend=True,\n              palette='flare',alpha=1.0, label='Below Mean')\n\nsns.swarmplot(x='scored_signif',y='country',\n              data=combined_outliers_df, legend=False,\n              marker=\"x\", linewidth=2.0, label='(Removed) Outliers')\n\nplt.xlabel(\"Score Significance\")\nplt.ylabel(\"Country\")\n\nlegend_handles = [\n    mlines.Line2D([], [], color='red', label='Mean & Above', marker='o', markersize=4, linestyle='None'),\n    mlines.Line2D([], [], color='blue', label='Below Mean', marker='o', markersize=4, linestyle='None'),\n    mlines.Line2D([], [], color='black', label='(Remove) Outliers', marker='x', markersize=4, linestyle='None')\n]\n\nplt.legend(handles=legend_handles, loc='upper right', bbox_to_anchor=(1.0, 0.75), fontsize='small')\n\ntitle_text = r'$\\bf{Score}$' + ' ' + r'$\\bf{Significance}$' + '\\nDistribution by Country (from 2021-23)'\nplt.title(title_text, fontsize=12)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:11.417868Z","iopub.execute_input":"2023-09-20T08:02:11.418163Z","iopub.status.idle":"2023-09-20T08:02:17.969813Z","shell.execute_reply.started":"2023-09-20T08:02:11.418137Z","shell.execute_reply":"2023-09-20T08:02:17.968848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\n\nsns.kdeplot(\n    data=combined_drama_df, x='scored_signif', hue='country', \n    fill=True, common_norm=False,\n    palette='Set2', alpha=0.5, linewidth=0\n)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:17.971012Z","iopub.execute_input":"2023-09-20T08:02:17.971315Z","iopub.status.idle":"2023-09-20T08:02:18.373879Z","shell.execute_reply.started":"2023-09-20T08:02:17.971288Z","shell.execute_reply":"2023-09-20T08:02:18.372754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hypothesis Testing:\n","metadata":{}},{"cell_type":"code","source":"df = combined_drama_df.copy()\nkorean_data = df[df['country'] == 'South Korea']['scored_signif']\nkorean_data","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:18.374966Z","iopub.execute_input":"2023-09-20T08:02:18.375429Z","iopub.status.idle":"2023-09-20T08:02:18.386036Z","shell.execute_reply.started":"2023-09-20T08:02:18.375402Z","shell.execute_reply":"2023-09-20T08:02:18.385299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hypothese testing for South Korea vs. Thailand","metadata":{}},{"cell_type":"code","source":"from scipy import stats\n\nhypothesis_text = f\"\"\"Null Hypothesis (H0): The average score significance of Korean drama is equal to or less than that of Thai drama.\nAlternative Hypothesis (H1): The average score significance of Korean drama is greater than that of Thai drama.\n\"\"\"\nprint(hypothesis_text)\n\ndf = combined_drama_df.copy()\nkorean_data = df[df['country'] == 'South Korea']['scored_signif']\nthai_data = df[df['country'] == 'Thailand']['scored_signif']\n\n# Perform a two-sample t-test\nt_statistic, p_value = stats.ttest_ind(korean_data, thai_data, alternative='greater')\n\n# Set the significance level (alpha)\nalpha = 0.05\n\n# Print the results\nprint(f'T-Statistic: {t_statistic}')\nprint(f'P-Value: {p_value}')\n\nif p_value < alpha:\n    print(\"Reject the null hypothesis: The average score significance of Korean drama is greater than that of Thai drama.\")\nelse:\n    print(\"Fail to reject the null hypothesis: There is not enough evidence to conclude that the average score significance of Korean drama is greater than that of Thai drama.\")\n\n# Create PDFs for both groups\nplt.figure(figsize=(10, 6))\nsns.kdeplot(korean_data, label='Korean', shade=True)\nsns.kdeplot(thai_data, label='Thai', shade=True)\n\nplt.xlabel(\"Score Significance\")\nplt.ylabel(\"Probability Density\")\nplt.title(\"Probability Density Plot for T-Test\")\n\n# Add a legend and annotate p-value\nplt.legend()\nplt.annotate(f'P-Value: {p_value:.4f}', xy=(3.5, 0.4), fontsize=12, color='red')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:18.387163Z","iopub.execute_input":"2023-09-20T08:02:18.387423Z","iopub.status.idle":"2023-09-20T08:02:18.818190Z","shell.execute_reply.started":"2023-09-20T08:02:18.387401Z","shell.execute_reply":"2023-09-20T08:02:18.817539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hypothesis Testing for South Korea vs. Japan","metadata":{}},{"cell_type":"code","source":"from scipy import stats\n\nhypothesis_text = f\"\"\"Null Hypothesis (H0): The average score significance of Korean drama is equal to or less than that of Japanese drama.\nAlternative Hypothesis (H1): The average score significance of Korean drama is greater than that of Japanese drama.\n\"\"\"\nprint(hypothesis_text)\n\ndf = combined_drama_df.copy()\nkorean_data = df[df['country'] == 'South Korea']['scored_signif']\nthai_data = df[df['country'] == 'Japan']['scored_signif']\n\n# Perform a two-sample t-test\nt_statistic, p_value = stats.ttest_ind(korean_data, thai_data, alternative='greater')\n\n# Set the significance level (alpha)\nalpha = 0.05\n\n# Print the results\nprint(f'T-Statistic: {t_statistic}')\nprint(f'P-Value: {p_value}')\n\nif p_value < alpha:\n    print(\"Reject the null hypothesis: The average score significance of Korean drama is greater than that of Japanese drama.\")\nelse:\n    print(\"Fail to reject the null hypothesis: There is not enough evidence to conclude that the average score significance of Korean drama is greater than that of Japanese drama.\")\n\n# Create PDFs for both groups\nplt.figure(figsize=(10, 6))\nsns.kdeplot(korean_data, label='Korean', shade=True)\nsns.kdeplot(thai_data, label='Thai', shade=True)\n\nplt.xlabel(\"Score Significance\")\nplt.ylabel(\"Probability Density\")\nplt.title(\"Probability Density Plot for T-Test\")\n\n# Add a legend and annotate p-value\nplt.legend()\nplt.annotate(f'P-Value: {p_value:.4f}', xy=(3.5, 0.4), fontsize=12, color='red')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:18.819199Z","iopub.execute_input":"2023-09-20T08:02:18.819611Z","iopub.status.idle":"2023-09-20T08:02:19.245446Z","shell.execute_reply.started":"2023-09-20T08:02:18.819587Z","shell.execute_reply":"2023-09-20T08:02:19.244652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ANOVA: Is there a significant difference in the means of the groups (South Kore, Thailand, and Japan)","metadata":{}},{"cell_type":"code","source":"# Separate data into groups based on 'country'\ngrouped_data = [df['scored_signif'][df['country'] == country] for country in df['country'].unique()]\n\n# Perform one-way ANOVA\nf_statistic, p_value = stats.f_oneway(*grouped_data)\n\n# Set the significance level (alpha)\nalpha = 0.05\n\n# Print the results\nprint(f'F-Statistic: {f_statistic}')\nprint(f'P-Value: {p_value}')\n\nif p_value < alpha:\n    print(\"Reject the null hypothesis: There is a significant difference in the means of at least two groups.\")\nelse:\n    print(\"Fail to reject the null hypothesis: There is no significant difference in the means of the groups.\")\n\nplt.figure(figsize=(10, 6))\nsns.kdeplot(grouped_data[0], label='Korean', shade=True)\nsns.kdeplot(grouped_data[1], label='Thai', shade=True)\nsns.kdeplot(grouped_data[2], label='Japanese', shade=True)\n\nplt.xlabel(\"Score Significance\")\nplt.ylabel(\"Probability Density\")\nplt.title(\"Probability Density Plot for ANOVA\")\n\n# Add a legend and annotate p-value\nplt.legend()\nplt.annotate(f'P-Value: {p_value:.4f}', xy=(3.5, 0.4), fontsize=12, color='red')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:19.246592Z","iopub.execute_input":"2023-09-20T08:02:19.247057Z","iopub.status.idle":"2023-09-20T08:02:19.707540Z","shell.execute_reply.started":"2023-09-20T08:02:19.247029Z","shell.execute_reply":"2023-09-20T08:02:19.706681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------","metadata":{}},{"cell_type":"markdown","source":"# We might disregard this \"Review\" section since we couldn't get much of the information?\n\n## Reviews\n\nReviews can give a more indept analysis on why these drama are so great. However, not every drama has the same number of comments. In this case, we will just do the analysis based on the existing reviews. \n\nFor each review, the scoring can be seperated into mutliple sub-scoring; hence, each field provides more insight!","metadata":{}},{"cell_type":"markdown","source":"#### Preprocessing Task\nWe will preprocess that data. Firstly, we defined a function called `review_preprocess_pipeline` that extract useful information such as total number of episode the reviewer watched, which we can used to calculated the `watched_ratio`, This ratio will be used as a weighting factor to find weighted average rating score or `wrat_avg_score` reviewers gave to the drama.\n\n#### Why should we consider using a weighted score in our analysis? \nTo illustrate this, let's draw a parallel with shopping on an e-commerce website. Imagine you come across two identical products being sold by two different vendors. In most cases, you would likely opt to purchase from the vendor with a stronger reputation. This reputation can manifest in various ways: a higher number of followers, likes, or exceptionally high average review scores, such as a perfect 5.0 stars.\n\nNow, let's apply a similar line of thinking to our analysis. When it comes to reviews of a particular drama, we value input from individuals with greater credibility. This credibility comes from those who have already watched the entire drama, as they can provide a more comprehensive and informed perspective on it. Therefore, we may choose to assign more weight to reviews from viewers with a proven track record of completing the drama, just as we would preferentially buy from a trusted vendor when shopping online.","metadata":{}},{"cell_type":"markdown","source":"### Probability Problem: \nGiven that we picked one Lakorn Thai randomly, what is the probability that the Lakorn is top rated and belongs to Horror, Romance or  ","metadata":{}},{"cell_type":"code","source":"def review_preprocess_pipeline(\n    df: pd.DataFrame,\n    country: str\n) -> pd.DataFrame:\n    \n    # Filtered for review that is valid\n    df = df[df['title'].isin(valid_drama_dict[country])].iloc[:,1:]\n    \n    # Create Additional Features into each Dataset as a Key to determine the associated Country of the Drama being commented\n    df['country'] = country\n    \n    # Removed the people that watched more than the number of episodes?\n    \n    # Extract Number of Epsiode watched and Total Episode Number\n    df['ep_watched'] = df['ep_watched'].fillna('0 of 0 episodes seen')\n    df['tot_watched'] = df['ep_watched'].apply(lambda x: int(str(x).split(' ')[0]))\n    df['tot_ep'] = df['ep_watched'].apply(lambda x: int(str(x).split(' ')[2]))\n    \n    # Feature Engineering\n    df['ep_watched_ratio'] = df['tot_watched'] / df['tot_ep']\n    df['ep_watched_ratio'] = df['ep_watched_ratio'].fillna(0.0).apply(lambda x: round(x,3))\n    \n    df['avg_score'] = df[['story','acting_cast','music','rewatch_value']].mean(axis=1)\n    df['wrat_avg_score'] = df['avg_score'] * df['ep_watched_ratio']\n    \n    # Remove Features that wouldn't be part of the analysis \n    df = df.drop(columns=['ep_watched', 'text'], axis=1)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:19.708924Z","iopub.execute_input":"2023-09-20T08:02:19.709528Z","iopub.status.idle":"2023-09-20T08:02:19.718078Z","shell.execute_reply.started":"2023-09-20T08:02:19.709499Z","shell.execute_reply":"2023-09-20T08:02:19.717082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rtha_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/tha_user_reviews.csv')\nrkor_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/kor_user_reviews.csv')\nrjap_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/jap_user_reviews.csv')\n\nrtha_df_f = review_preprocess_pipeline(rtha_df,'Thailand')\nrkor_df_f = review_preprocess_pipeline(rkor_df, 'South Korea')\nrjap_df_f = review_preprocess_pipeline(rjap_df, 'Japan')\n\nrtha_df_f.shape, rkor_df_f.shape, rjap_df_f.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:19.719483Z","iopub.execute_input":"2023-09-20T08:02:19.719823Z","iopub.status.idle":"2023-09-20T08:02:20.218949Z","shell.execute_reply.started":"2023-09-20T08:02:19.719792Z","shell.execute_reply":"2023-09-20T08:02:20.218305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rtha_outliers, rtha_df_filtered = process_outliers_df_pipeline(rtha_df_f, 'wrat_avg_score')\nrkor_outliers, rkor_df_filtered = process_outliers_df_pipeline(rkor_df_f, 'wrat_avg_score')\nrjap_outliers, rjap_df_filtered = process_outliers_df_pipeline(rjap_df_f, 'wrat_avg_score')","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:20.219854Z","iopub.execute_input":"2023-09-20T08:02:20.220517Z","iopub.status.idle":"2023-09-20T08:02:20.235444Z","shell.execute_reply.started":"2023-09-20T08:02:20.220489Z","shell.execute_reply":"2023-09-20T08:02:20.234414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rtha_df_filtered.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:20.237755Z","iopub.execute_input":"2023-09-20T08:02:20.237995Z","iopub.status.idle":"2023-09-20T08:02:20.255518Z","shell.execute_reply.started":"2023-09-20T08:02:20.237974Z","shell.execute_reply":"2023-09-20T08:02:20.254494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_boxplots_with_annotations(\n    dataframe: pd.DataFrame,\n    country: str = '',\n) -> None:\n    sns.reset_orig()\n    \n    num_columns = len(dataframe.columns)\n    num_rows = (num_columns + 2) // 3\n    \n    fig, axes = plt.subplots(nrows=num_rows, ncols=3, figsize=(16, 6 * num_rows // 2))\n    axes = axes.flatten()\n\n    for i, col in enumerate(dataframe.columns):\n        # Create a boxplot for the current column in the corresponding subplot\n        ax = axes[i]\n        ax.boxplot(dataframe[col], vert=False, sym='b.', \n                   whis=1.5, patch_artist=True, boxprops=dict(facecolor='lightblue'))\n\n        # Outlier Calculation using IQR Method\n        q1 = np.percentile(dataframe[col], 25)\n        q3 = np.percentile(dataframe[col], 75)\n        iqr = q3 - q1\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n        outliers = [x for x in dataframe[col] if x < lower_bound or x > upper_bound]\n        num_outliers = len(outliers)\n        \n        mean = np.mean(dataframe[col])\n        std_dev = np.std(dataframe[col])\n        annotation = f\"Mean: {mean:.2f}\\nStd Dev: {std_dev:.2f}\\nOutliers: {num_outliers}\"\n        ax.text(0.62, 0.80, annotation, transform=ax.transAxes, fontsize=12,\n                verticalalignment='center')\n        \n        min_score = np.min(dataframe[col])\n        max_score = np.max(dataframe[col])\n        minmax_annotation = f\"Min: {min_score:.2f}\\nMax: {max_score:.2f}\"\n        ax.text(0.62, 0.25, minmax_annotation, transform=ax.transAxes, fontsize=12,\n               verticalalignment='center')\n\n        ax.set_title(col)\n    \n    big_title = f\"{country} Boxplots\"\n    fig.suptitle(big_title, fontsize=16, y=1.02)\n    \n    for j in range(num_columns, len(axes)):\n        fig.delaxes(axes[j])\n        \n    plt.tight_layout()\n    plt.show()\n\n# features = [\n#     'story', 'acting_cast', 'music', 'rewatch_value', 'overall',\n#     'n_helpful', 'tot_watched', 'tot_ep', 'ep_watched_ratio',\n#     'avg_score', 'wrat_avg_score'\n# ]\n\nfeatures = ['ep_watched_ratio','overall','wrat_avg_score']\n\ndraw_boxplots_with_annotations(rtha_df_filtered[features], 'Thailand')\n\ndraw_boxplots_with_annotations(rkor_df_filtered[features], 'South Korea')\n\ndraw_boxplots_with_annotations(rjap_df_filtered[features], 'Japan')","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:20.257022Z","iopub.execute_input":"2023-09-20T08:02:20.257759Z","iopub.status.idle":"2023-09-20T08:02:21.599822Z","shell.execute_reply.started":"2023-09-20T08:02:20.257726Z","shell.execute_reply":"2023-09-20T08:02:21.599004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do we have to manage the outliers based on ep_watched_ratio, n_helpful, etc.?\n\nWhether to remove the outliers before or after performing a feature engineering like weighted averge score depends on the specific golas and requirement of this analysis. Upon observation of the fields related to score, we notice some outliers. However, this is something we would like to capture. The minimum and maximum values for these fields are between 1 and 10; hence, there is no hidden error in the data extraction or scoring process on the website. They are valuable insights even if they were outliers; henceforth, we won't remove it for this moment. \n\n\nAddress other features and how does outlier in those feature effect the final weighted score\n","metadata":{}},{"cell_type":"code","source":"print(\"Removed based the the Episode Watched Ratio: \")\n_, rtha_df_f2 = process_outliers_df_pipeline(rtha_df_filtered, 'ep_watched_ratio')\n_, rkor_df_f2 = process_outliers_df_pipeline(rkor_df_filtered, 'ep_watched_ratio')\n_, rjap_df_f2 = process_outliers_df_pipeline(rjap_df_filtered, 'ep_watched_ratio')\n\nprint(\"Removed based on Weigthed Ratio Average Score: \")\n_, rtha_df_f2 = process_outliers_df_pipeline(rtha_df_f2, 'wrat_avg_score')\n_, rkor_df_f2 = process_outliers_df_pipeline(rtha_df_f2, 'wrat_avg_score')\n_, rjap_df_f2 = process_outliers_df_pipeline(rtha_df_f2, 'wrat_avg_score')\n\n\n\nfeatures = ['ep_watched_ratio','overall','wrat_avg_score']\n\ndraw_boxplots_with_annotations(rtha_df_f2[features], 'Thailand')\n\ndraw_boxplots_with_annotations(rkor_df_f2[features], 'South Korea')\n\ndraw_boxplots_with_annotations(rjap_df_f2[features], 'Japan')","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:21.601255Z","iopub.execute_input":"2023-09-20T08:02:21.601728Z","iopub.status.idle":"2023-09-20T08:02:22.810830Z","shell.execute_reply.started":"2023-09-20T08:02:21.601692Z","shell.execute_reply":"2023-09-20T08:02:22.809913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rtha_df_f2[rtha_df_f2['title'] == '10 Years Ticket']","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:22.812475Z","iopub.execute_input":"2023-09-20T08:02:22.812838Z","iopub.status.idle":"2023-09-20T08:02:22.831448Z","shell.execute_reply.started":"2023-09-20T08:02:22.812803Z","shell.execute_reply":"2023-09-20T08:02:22.830548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing the aggregate function\nagg_rtha_df = (\n    rtha_df_f2\n    .reset_index()\n    .groupby(['title'])\n    .agg({'wrat_avg_score':'mean', 'index':'count'})\n    .rename(columns={'wrat_avg_score': 'avr_wrat_score', 'index':'num_reviews'})\n    .reset_index()\n)\n\nagg_rtha_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:22.832570Z","iopub.execute_input":"2023-09-20T08:02:22.832831Z","iopub.status.idle":"2023-09-20T08:02:22.852942Z","shell.execute_reply.started":"2023-09-20T08:02:22.832808Z","shell.execute_reply":"2023-09-20T08:02:22.852128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----","metadata":{}},{"cell_type":"markdown","source":"## Actors Processing","metadata":{}},{"cell_type":"code","source":"atha_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/tha_tha_actors.csv')\nakor_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/kor_kor_actors.csv')\najap_df = pd.read_csv('/kaggle/input/thai-and-japanese-drama-vs-korean-drama-dominance/jap_jap_actors.csv')\n\natha_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:22.853971Z","iopub.execute_input":"2023-09-20T08:02:22.854299Z","iopub.status.idle":"2023-09-20T08:02:22.931484Z","shell.execute_reply.started":"2023-09-20T08:02:22.854236Z","shell.execute_reply":"2023-09-20T08:02:22.930628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function to check uniqueness of the actor name\ndef check_actor_name_uniqueness(df: pd.DataFrame) -> None:\n    \n    actor_dict = {}\n    \n    for index, row in df.iterrows():\n        actor_id, actor_name = row['actor_id'], row['actor_name']\n\n        if actor_name not in actor_dict.keys():\n            actor_dict[actor_name] = {\n                'id_list': []\n            }\n\n        if actor_id not in actor_dict[actor_name]['id_list']:\n            actor_dict[actor_name]['id_list'].append(actor_id)\n\n    for key, val in actor_dict.items():\n        actor_dict[key]['nunique'] = len(actor_dict[key]['id_list'])\n\n        if actor_dict[key]['nunique'] != 1:\n            print(\"Actor name isn't unique!\")\n            return \n    \n    print(\"Actor name is UNIQUE!\")\n    return \n\n# Check the uniqueness of the actor name. Need to use them as key later-on\ncheck_actor_name_uniqueness(atha_df)\ncheck_actor_name_uniqueness(akor_df)\ncheck_actor_name_uniqueness(ajap_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:22.933177Z","iopub.execute_input":"2023-09-20T08:02:22.933851Z","iopub.status.idle":"2023-09-20T08:02:24.760402Z","shell.execute_reply.started":"2023-09-20T08:02:22.933817Z","shell.execute_reply":"2023-09-20T08:02:24.759722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write a function to merge the information of actor and drama (might have to be the new dataframe created from the processed reviews?) to get the information regarding the drama performance that each actor acted in. If have to use the processed data from review, then move code blocks related to actors to be after the user reviews sections","metadata":{}},{"cell_type":"code","source":"df = atha_df.copy()\n\ndrama_df = dtha_df_filtered[['drama_name','genres','tags','country','content_rt','scored_signif']].copy()\ncountry = 'Thailand'\n\ndf = df.drop(columns=['actor_id','character_name'],axis=1)\ndf = df[df['drama_name'].isin(valid_drama_dict[country])]\n\nmerged_df = df.merge(drama_df, on='drama_name', how='left')\n\nmerged_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:24.761314Z","iopub.execute_input":"2023-09-20T08:02:24.761991Z","iopub.status.idle":"2023-09-20T08:02:24.787898Z","shell.execute_reply.started":"2023-09-20T08:02:24.761965Z","shell.execute_reply":"2023-09-20T08:02:24.787064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df['role'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:24.788959Z","iopub.execute_input":"2023-09-20T08:02:24.789226Z","iopub.status.idle":"2023-09-20T08:02:24.795094Z","shell.execute_reply.started":"2023-09-20T08:02:24.789201Z","shell.execute_reply":"2023-09-20T08:02:24.794328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nDictionary Architectures:\n\nactor_name\n    - role\n        - genres\n        - tags\n        - content_rt\n    - appearance\n\nval: scored_signig\n'''\n# Convert this dictionary into dataframe\nactor_meta_db = {}\n\n# Focus Only on Main Role and Support Role\nsdf = merged_df[merged_df['role'].isin(['Main Role','Support Role'])]\n\nfor index, row in sdf.iterrows():\n    \n    name, role, score = row['actor_name'], row['role'], row['scored_signif']\n    \n    if name not in actor_meta_db.keys():\n        actor_meta_db[name] = {\n            'Main Role': {},\n            'Support Role': {}\n        }\n\n    for genre in row['genres']:\n        if genre != 'nan' and genre not in actor_meta_db[name][role].keys():\n            actor_meta_db[name][role][genre] = []\n            \n        if genre in actor_meta_db[name][role].keys():\n            actor_meta_db[name][role][genre].append(score)\n\nfor key, item in actor_meta_db.items():\n\n    for role in ['Main Role', 'Support Role']:\n        if item[role].keys():\n            for genre, score_list in item[role].items():\n                average_score = np.mean(score_list)\n\n                actor_meta_db[key][role][genre] = round(average_score,3)\n    \n    \nimport json\n# print(json.dumps(actor_meta_db, indent=4))","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:02:24.796069Z","iopub.execute_input":"2023-09-20T08:02:24.796415Z","iopub.status.idle":"2023-09-20T08:02:25.159111Z","shell.execute_reply.started":"2023-09-20T08:02:24.796390Z","shell.execute_reply":"2023-09-20T08:02:25.158318Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nDictionary Architectures:\n\nactor_name\n    - role\n        - genres\n        - tags\n        - content_rt\n    - appearance\n\nval: scored_signig\n'''\n# For Tags\nactor_meta_db = {}\n\n# Focus Only on Main Role and Support Role\nsdf = merged_df[merged_df['role'].isin(['Main Role','Support Role'])]\n\nfor index, row in sdf.iterrows():\n    \n    name, role, score = row['actor_name'], row['role'], row['scored_signif']\n    \n    if name not in actor_meta_db.keys():\n        actor_meta_db[name] = {\n            'Main Role': {},\n            'Support Role': {}\n        }\n    \n    for tag in row['tags']:\n        if tag != \"(Vote or add tags)\" and tag not in actor_meta_db[name][role].keys():\n            actor_meta_db[name][role][tag] = []\n\n        if tag in actor_meta_db[name][role].keys():\n            actor_meta_db[name][role][tag].append(score)\n\nfor key, item in actor_meta_db.items():\n\n    for role in ['Main Role', 'Support Role']:\n        if item[role].keys():\n            for tag, score_list in item[role].items():\n                average_score = np.mean(score_list)\n\n                actor_meta_db[key][role][tag] = round(average_score,3)\n    \n    \nimport json\n# print(json.dumps(actor_meta_db, indent=4))","metadata":{"execution":{"iopub.status.busy":"2023-09-20T08:03:12.546099Z","iopub.execute_input":"2023-09-20T08:03:12.546476Z","iopub.status.idle":"2023-09-20T08:03:13.238078Z","shell.execute_reply.started":"2023-09-20T08:03:12.546441Z","shell.execute_reply":"2023-09-20T08:03:13.237428Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}